```python

import logging
# Suppress unnecessary scapy runtime warnings
logging.getLogger("scapy.runtime").setLevel(logging.ERROR)

# Import required libraries
import argparse
from pathlib import Path
from scapy.all import *  # Import all scapy functions and protocols
from scapy.all import TCP  # Specifically import TCP layer

# Define the logging format
format_str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Function to extract HTTP headers from the HTTP payload
def get_http_headers(http_payload):
    try:
        # Extract headers by searching for the "\r\n\r\n" delimiter, which separates headers from body
        headers_raw = http_payload[:http_payload.index(b"\r\n\r\n") + 2]
        headers = dict(re.findall(b"(?P<name>.*?): (?P<value>.*?)\\r\\n", headers_raw))
    except ValueError as err:
        logging.error('Could not find \\r\\n\\r\\n - %s' % err)
        return None
    except Exception as err:
        logging.error('Exception found trying to parse raw headers - %s' % err)
        logging.debug(str(http_payload))
        return None

    # Ensure Content-Type header is present
    if b"Content-Type" not in headers:
        logging.debug('Content Type not present in headers')
        logging.debug(headers.keys())
        return None   
    return headers

# Function to extract objects from the HTTP payload based on the headers
def extract_object(headers, http_payload):
    object_extracted = None
    object_type = None

    # Define filters for object types
    content_type_filters = [b'application/x-msdownload', b'application/octet-stream']

    try:
        # If Content-Type header exists, check for matching types in the filters
        if b'Content-Type' in headers.keys():
            if headers[b'Content-Type'] in content_type_filters:
                object_extracted = http_payload[http_payload.index(b"\r\n\r\n") + 4:]  # Extract object after headers
                object_type = object_extracted[:2]  # Get the first two bytes as the object type
                logging.info("Object Type: %s" % object_type)
            else:
                logging.debug('Content Type did not match with filters - %s' % headers[b'Content-Type'])
                if len(http_payload) > 10:
                    logging.debug('Object first 50 bytes - %s' % str(http_payload[:50]))
        else:
            logging.info('No Content Type in Package')
            logging.debug(headers.keys())

        # If Content-Length exists, log it
        if b'Content-Length' in headers.keys():
            logging.info("%s: %s" % (b'Content-Length', headers[b'Content-Length']))

    except Exception as err:
        logging.error('Exception found trying to parse headers - %s' % err)
        return None, None

    return object_extracted, object_type

# Create an output directory to save extracted objects
def create_output_directory_folder(directory_name, output_directory='objects') -> str:
    if not os.path.exists(output_directory):
        logging.debug('Directory %s does not exist - creating' % output_directory)
        os.mkdir(output_directory)
    
    directory_name = directory_name.replace('.pcap', '')  # Remove the '.pcap' from the filename
    target_path = os.path.join(os.getcwd(), output_directory, directory_name)
    
    if not os.path.exists(target_path):
        logging.debug('Path %s does not exist - creating.' % target_path)
        os.mkdir(target_path)
    
    return target_path

# Parse the filename from the pcap file path
def parse_pcap_filename(pcap_file) -> str:
    parts = pcap_file.split('/')
    logging.debug('Pcap File path %s - Parts %d' % (pcap_file, len(parts)))
    if len(parts) > 1:
        return parts[-1]
    else:
        return parts[0]

# Extract HTTP objects from the given pcap file and save them
def extract_http_objects(pcap_file, output_directory):
    logging.info('Starting to parse pcap/s')

    filtered_object_types = [b'MZ']  # Filter for object types (e.g., executable files)
    pcap_file_name = parse_pcap_filename(pcap_file)  # Get the filename without path
    pcap_flow = rdpcap(pcap_file)  # Read the pcap file
    target_directory = create_output_directory_folder(pcap_file_name, output_directory)

    sessions = pcap_flow.sessions()  # Get all sessions in the pcap file

    objects_count = 0  # Count of found objects
    objects_saved = 0  # Count of saved objects

    for session in sessions:
        http_payload = bytes()  # Initialize an empty byte string for storing HTTP payload

        # Iterate through each packet in the session
        for packet in sessions[session]:
            if packet.haslayer(TCP):  # Check if the packet has a TCP layer
                # Capture HTTP (port 80) and HTTPS (port 443) traffic
                if packet[TCP].dport == 80 or packet[TCP].sport == 80:
                    if packet[TCP].payload:
                        payload = packet[TCP].payload
                        http_payload += raw(payload)  # Append HTTP payload to the buffer
                if packet[TCP].dport == 443 or packet[TCP].sport == 443:
                    logging.debug('HTTPS traffic detected')

        if len(http_payload):
            headers = get_http_headers(http_payload)

            if headers is None:
                continue  # Skip if headers could not be parsed

            logging.debug("HTTP Payload length: %d" % len(http_payload))
            object_found, object_type = extract_object(headers, http_payload)

            if object_found is not None and object_type is not None:
                objects_count += 1  # Increment object count

                if len(object_found) == 0:
                    logging.debug("Object found with length 0")
                    continue  # Skip if object has length 0

                if object_type not in filtered_object_types:
                    logging.debug("Non parseable Content Type %s" % (object_type))
                    continue  # Skip unsupported object types

                object_name = "%s_object_found_%d" % (pcap_file_name, objects_count)

                # Save the extracted object to the target directory
                with open(f"{target_directory}/{object_name}", "wb") as fd:
                    fd.write(object_found)
                objects_saved += 1
            elif object_found:
                logging.debug('Object found length: %d' % len(object_found))
            elif object_type:
                logging.debug('Object Type: %d' % object_type)

    # Log the final statistics
    logging.info('Parsed all files')
    logging.info("Total Number of Objects Found: %d" % (objects_count))
    logging.info("Total Number of Objects Saved: %d" % (objects_saved))

# Extract HTTP objects from all pcap files in a directory
def extract_http_objects_from_directory(target_directory, output_directory):
    # List all files in the directory
    directory_files = os.listdir(target_directory)
    logging.debug('Target directory has %d files for extraction' % len(directory_files))

    for target_file in directory_files:
        print(target_file)
        # If the file is a pcap file, parse it
        if Path(target_file).suffix == '.pcap':
            logging.debug('New pcap file to parse %s' % target_file)
            extract_http_objects(os.path.join(target_directory, target_file), output_directory)
        else:
            logging.debug('Not a pcap file %s' % Path(target_file).suffix)

    logging.info('All files parsed')

# Function to print help message for usage
def print_help():
    print("python pcap_file_extraction.py --inputpcap <file>")

# Main function to parse arguments and execute logic
def main():
    # Set up command-line argument parser
    parser = argparse.ArgumentParser(description="Parse pcap and extract files")
    parser.add_argument('-i', '--inputpcap', required=True, help='PCAP file or Directory to process files')
    parser.add_argument('-o', '--outputdir', default='objects', type=str, help='Output Directory where to place the Extracted files')
    parser.add_argument('-d', '--debug', help='Enable Debugging Logging', action='store_const', dest='loglevel', const=logging.DEBUG, default=logging.INFO)
    parser.add_argument('-l', '--log', help='Specify Log File', dest='logfile', type=str, default='extractor.log')

    args = parser.parse_args()

    # Set up logging configuration
    logging.basicConfig(filename=args.logfile, format=format_str, level=args.loglevel)

    logging.info("Starting up")
    if args.inputpcap:
        if os.path.isfile(args.inputpcap):
                print('Parsing file - %s' % args.inputpcap)
                extract_http_objects(args.inputpcap, args.outputdir)  # Process single pcap file
        elif os.path.isdir(args.inputpcap):
                print('Parsing Directory - %s' % args.inputpcap)
                extract_http_objects_from_directory(args.inputpcap, args.outputdir)  # Process pcap files in a directory

    logging.info('Finishing up')

# Entry point of the script
if __name__ == "__main__":
    main()

```
